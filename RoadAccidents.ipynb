{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Advance Data Science Specialization\n",
    "## Capstone Project\n",
    "### Determining The Accuracy Of Machine Learning Model;\n",
    "### Trained To Predict The Rate Of Mortality Per 100,000 Population On Roads, Globally. \n",
    "In this project we will train a machine learning model to predict the rate of mortality per 100,000 population on roads, globally. We will accomplish that by feeding the data for the Road Safety Previous Decade of Action and a projected data for the next Road Safety Decade of Action. Then we will train the model and evaluate its accuracy. Finally, we will predict the rate of mortality for the year 2030.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This Notebook works best on a Python Spark Machine !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We check that tensorflow and Spark is already installed on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We import all the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "import pandas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sys\n",
    "from queue import Queue\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We get the data required for training the model. In this case, the rate of mortality per 100,000 of the population for the previous and next decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/IjA1020/IBM-Project/main/previous.csv\"\n",
    "download = requests.get(url).content\n",
    "DataFrame1 = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "dataset1 = DataFrame1.values\n",
    "previous = dataset1\n",
    "X1 = dataset1[:,0].astype(float)\n",
    "Y1 = dataset1[:,1]\n",
    "print (DataFrame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/IjA1020/IBM-Project/main/next.csv\"\n",
    "download = requests.get(url).content\n",
    "DataFrame2 = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "dataset2 = DataFrame2.values\n",
    "next = dataset2\n",
    "X2 = dataset2[:,0].astype(float)\n",
    "Y2 = dataset2[:,1]\n",
    "print (DataFrame2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now we plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.plot(X1, Y1, lw=2)\n",
    "ax.set_xlabel(\"YEAR\")\n",
    "ax.set_ylabel(\"CASUALTIES\")\n",
    "ax.set_title(\"Previous Decade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.plot(X2, Y2, lw=2)\n",
    "ax.set_xlabel(\"YEAR\")\n",
    "ax.set_ylabel(\"CASUALTIES\")\n",
    "ax.set_title(\"previous Decade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we convert the data into frequencies through Fast Fourier Transform (FFT) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_fft = np.fft.fft(previous).real\n",
    "next_fft = np.fft.fft(next).real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We plot the frequencies and observe the trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(13, 5), dpi=60, facecolor='w', edgecolor='r')\n",
    "size = len(previous_fft)\n",
    "ax.plot(range(0,size), previous_fft[:,0].real, '-', color='green', animated = True, linewidth=3)\n",
    "ax.plot(range(0,size), previous_fft[:,1].real, '-', color='brown', animated = True, linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(13, 5), dpi=60, facecolor='w', edgecolor='r')\n",
    "size = len(next_fft)\n",
    "ax.plot(range(0,size), next_fft[:,0].real, '-', color='green', animated = True, linewidth=3)\n",
    "ax.plot(range(0,size), next_fft[:,1].real, '-', color='brown', animated = True, linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting The Data Into Machine Readable Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In order to train the model, we need to convert the data into machine learning algorithms, in other words 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_scaled = scaleData(previous_fft)\n",
    "next_scaled = scaleData(next_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_scaled = previous_scaled.T\n",
    "next_scaled = next_scaled.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we reshape the data into 2 rows and 10 columns. Each row represeting the frequency spectrums for each column of our initially uploaded data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_scaled.reshape(2, 10)\n",
    "next_scaled.reshape(2, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now we prepare the training data by concatenating a label “0” for the previous decade and a label “1” for the next decade and than we combine the two data sets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_previous = np.repeat(1,2)\n",
    "label_previous.shape = (2,1)\n",
    "label_next = np.repeat(0,2)\n",
    "label_next.shape = (2,1)\n",
    "\n",
    "train_previous = np.hstack((previous_scaled,label_previous))\n",
    "train_next = np.hstack((next_scaled,label_next))\n",
    "train_both = np.vstack((train_previous,train_next))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Just to check that the data has been successfully converted, we plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Model\n",
    "\n",
    "Now that we have our data in machine readable algorithms, we can easily create our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firstly, we need to define our parameters. We can do that through the array slicing syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = train_both[:,0:-1]\n",
    "labels = train_both[:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the loss model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        sys.stdout.write(str(logs.get('loss'))+str(', '))\n",
    "        sys.stdout.flush()\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "        \n",
    "learning_rate = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(10, ), activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def train(data,label):\n",
    "    model.fit(rate, labels, epochs=50, batch_size=20, verbose=0, shuffle=True, validation_data=(data, label), callbacks=[learning_rate])\n",
    "\n",
    "\n",
    "def score(data):\n",
    "    return model.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now we can train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(rate, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. See the model summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(previous_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(next_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate our model and print accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(features, labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('road_accidents.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(13, 5), dpi=60, facecolor='w', edgecolor='k')\n",
    "size = len(learning_rate.losses)\n",
    "ax.plot(range(0,size), learning_rate.losses, '-', color='blue', animated = True, linewidth=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6213e0af081d56db034f795ff49d96980936e2ae540dda57fc6c3cbea6a5fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
